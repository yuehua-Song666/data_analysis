# Data pre-processing -- imbalanced data #
#### Resampling methods are designed to change the composition of a training dataset for an imbalanced classification task. ####
- Library: Imbalanced_Learn Library
	- [https://github.com/scikit-learn-contrib/imbalanced-learn](https://github.com/scikit-learn-contrib/imbalanced-learn "Imbalanced learn library")
- Methods
	- Select Examples to keep
		- Near Miss Undersampling
			- Paper: [https://www.site.uottawa.ca/~nat/Workshop2003/jzhang.pdf](https://www.site.uottawa.ca/~nat/Workshop2003/jzhang.pdf "KNN approach to unbalanced data distributions")
			- NearMiss-1 -- Select negative examples that are close to some of the positive examples. (In the paper they select negative examples whose average diatance to the three closest positive examples are the smallest.)
				- Images:
				- ![](https://github.com/yuehua-Song666/data_analysis/blob/main/data_processing/img/Imbalanced_data_examples.png) ![](https://github.com/yuehua-Song666/data_analysis/blob/main/data_processing/img/NearMiss1.png)
				- Disadvantages:
					- Negative examples selected in this method may not be evenly distributed around positive examples.
			- NearMiss-2 -- Select negative examples that are close to all the positive exsamples. (In the paper examples are selected based on their average distances to three farthest positive examples.)
				- Images:
				- ![](https://github.com/yuehua-Song666/data_analysis/blob/main/data_processing/img/Imbalanced_data_examples.png) ![](https://github.com/yuehua-Song666/data_analysis/blob/main/data_processing/img/NearMiss2.png)
				- Disadvantages:
					- Negative examples selected in this method are close to all positive examples and they may not be evenly distributed around positive examples.
			- NearMiss-3 -- Select a given number of the closest negative examples for each positive examples. This method guarantees every positive example is surrounded by some negative examples.  
				- Images:
				- ![](https://github.com/yuehua-Song666/data_analysis/blob/main/data_processing/img/Imbalanced_data_examples.png) ![](https://github.com/yuehua-Song666/data_analysis/blob/main/data_processing/img/NearMiss3.png)
				- Advantages:
					- Every positive example is sorrounded by some selected negative examples and they could evenly distributed.
				- Disadvantages:
					- Precision is high, but its recall is low.
			- Most distant method -- choose the negative examples whose average distances to the closest three positive examples are the farthest.
		- Condensed Nearest Neighbor Rule Undersampling (CNN)
			- An undersampling technique that seeks a subset of a collection of samples that results in no loss in model performance, referred to as a minimal consistent set.
			- The focus of this algorithm is those examples in the minority class along the decision boundary between the two class, specifically, those majority examples around the minority class examples
				- Images:
				- ![](https://github.com/yuehua-Song666/data_analysis/blob/main/data_processing/img/Imbalanced_data_examples.png) ![](https://github.com/yuehua-Song666/data_analysis/blob/main/data_processing/img/condensed_nearest_neighbour.png)
				
	- Select Examplse to delete
		- Select examples from the majority class to delete, including the popular Tomek Links method and the Edited Nearest Neighbors rule.
			- Tomek Links for Undersampling
				- Find pairs of examples, one from each class; they together have the smallest Euclidean distance to each other in feature space.
				- After delete the paris, we can get a clear boundary between classes
				- ![](https://github.com/yuehua-Song666/data_analysis/blob/main/data_processing/img/tomek.png)
				- ![](https://github.com/yuehua-Song666/data_analysis/blob/main/data_processing/img/Imbalanced_data_examples.png) ![](https://github.com/yuehua-Song666/data_analysis/blob/main/data_processing/img/TomekLinks.png)
				-  Distanges:
				- Disadvantages:
					- Although finding the ambiguous examples on the class boundary is useful. It is not a great undersampling technique.
					- In practice, the Tomek Links need to combine with other methods, such as Condensed Nearest Neighbour Rule. (The choice to combine Tomek Links and CNN is natural, as Tomek Links can be said to remove borderline and noisy instances, while CNN removes redundant instances.)
			-  Edited Nearest Neighbors Rule for Undersampling
				-  Finding ambiguours and noisy examples in the dataset is called Edited Nearest Neighbours, or sometimes ENN for short
				-  This rule involves using k=3 nearest neighbors to locate those examples in a dataset that are misclassified and that are then removed before a k=1 classification rule is applied.
				-  Images:
				-  ![](https://github.com/yuehua-Song666/data_analysis/blob/main/data_processing/img/Imbalanced_data_examples.png) ![](https://github.com/yuehua-Song666/data_analysis/blob/main/data_processing/img/EditedNearestNeighbour.png)
				-  Distanges:
					-  Given the small amount of undersampling performed, the change to the mass of majority examples is not obvious from the plot.
	- Combination of Keep and Delete Methods
		- One-Sided Selection for Undersampling
		- Neighborhood Cleaning Rule for Undersampling
