# Data pre-processing -- imbalanced data #
#### Resampling methods are designed to change the composition of a training dataset for an imbalanced classification task. ####
- Library: Imbalanced_Learn Library
	- [https://github.com/scikit-learn-contrib/imbalanced-learn](https://github.com/scikit-learn-contrib/imbalanced-learn "Imbalanced learn library")
- Methods
	- Select Examples to keep
		- Near Miss Undersampling
			- Paper: [https://www.site.uottawa.ca/~nat/Workshop2003/jzhang.pdf](https://www.site.uottawa.ca/~nat/Workshop2003/jzhang.pdf "KNN approach to unbalanced data distributions")
			- NearMiss-1 -- Select negative examples that are close to some of the positive examples. (In the paper they select negative examples whose average diatance to the three closest positive examples are the smallest.)
				- Images:
				- ![](https://github.com/yuehua-Song666/data_analysis/blob/main/data_processing/img/Imbalanced_data_examples.png) ![](https://github.com/yuehua-Song666/data_analysis/blob/main/data_processing/img/NearMiss1.png)
				- Disadvantages:
					- Negative examples selected in this method may not be evenly distributed around positive examples.
			- NearMiss-2 -- Select negative examples that are close to all the positive exsamples. (In the paper examples are selected based on their average distances to three farthest positive examples.)
				- Images:
				- ![](https://github.com/yuehua-Song666/data_analysis/blob/main/data_processing/img/Imbalanced_data_examples.png) ![](https://github.com/yuehua-Song666/data_analysis/blob/main/data_processing/img/NearMiss2.png)
				- Disadvantages:
					- Negative examples selected in this method are close to all positive examples and they may not be evenly distributed around positive examples.
			- NearMiss-3 -- Select a given number of the closest negative examples for each positive examples. This method guarantees every positive example is surrounded by some negative examples.  
				- Images:
				- ![](https://github.com/yuehua-Song666/data_analysis/blob/main/data_processing/img/Imbalanced_data_examples.png) ![](https://github.com/yuehua-Song666/data_analysis/blob/main/data_processing/img/NearMiss3.png)
				- Advantages:
					- Every positive example is sorrounded by some selected negative examples and they could evenly distributed.
				- Disadvantages:
					- Precision is high, but its recall is low.
			- Most distant method -- choose the negative examples whose average distances to the closest three positive examples are the farthest.
		- Condensed Nearest Neighbor Rule Undersampling (CNN)
			- An undersampling technique that seeks a subset of a collection of samples that results in no loss in model performance, referred to as a minimal consistent set.
			- The focus of this algorithm is those examples in the minority class along the decision boundary between the two class, specifically, those majority examples around the minority class examples
				- Images:
				- ![](https://github.com/yuehua-Song666/data_analysis/blob/main/data_processing/img/Imbalanced_data_examples.png) ![](https://github.com/yuehua-Song666/data_analysis/blob/main/data_processing/img/condensed_nearest_neighbour.png)
				
	- Select Examplse to delete
		- Select examples from the majority class to delete, including the popular Tomek Links method and the Edited Nearest Neighbors rule.
